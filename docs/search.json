[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the about page"
  },
  {
    "objectID": "posts/blog-post-1/index.html",
    "href": "posts/blog-post-1/index.html",
    "title": "Palmer Pengiuns",
    "section": "",
    "text": "Palmer Penguins\nThis blog post takes data collected about three species of penguin in Antarctica and creates a predictive model of species based on the island that the penguin was found on and the penguins culmen length and depth. To begin, the data is cleaned and then explored using plots and aggregations. This allows us to have some idea of which variables will likely be good predictors of species, our target variable. Once I had narrowed down the predictor variables, I trained models on all possible combinations of the variables that we had and chose the top two best scores. I tried out a few other models, a Random Forest and a Support Vector Machine, in addition to the initial Logistic Regression model. After playing around with these models a little bit to increase training accuracy, I ended up with a Random Forest with training accuracy 100%. Finally I evaluated my model using the test data and ended up with a final accuracy of 100% on the test data.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np \nfrom matplotlib import pyplot as plt\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nThe following code processes our data set into a form that we are able to use. It gets rid of the columns that won’t be helpful in prediction (the ID, date the sample was collected, name of the study, comments etc.) and drops rows with any values that are not in the correct form (na or “.”).\nWe transform the species data from strings to numbers using LabelEncoder(). Our machine learning models can only understand numbers so we switch the species into an encoding of numbers, in this case between 0 and 2.\nFinally, we take the data frame (not including the variable that we are predicting, species) and call pd.get_dummies. This changes any non-numerical values into dummy or indicator values that can be used in our training. For instance, the Sex variable is turned into Sex_FEMALE and Sex_MALE, each if which has a true or false value (0 or 1).\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nEXPLORE\n\nsns.scatterplot(data = train, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species')\n\n\n\n\n\n\n\n\nThis plot shows the Body Mass vs. Flipper Length by species. It appears that the Gentoo penguin has a much larger body mass and longer flipper length than the other two species. However, there is not as much of a distinction between the Chinstrap and Adelie penguins. It seems that the Flipper Length is what distinguishes the Gentoo penguins from the other two species, as the Body Mass overlaps more between the three. This visualization indicates that flipper length may be a helpful variable in predicting species.\n\nsns.countplot(train, x=\"Island\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis graph shows the distribution of penguins on the three islands. It appears that Adelie penguins are found on all three islands, and are the only species of penguins found on Torgsen. Chinstrap penguins are only found on Dream and Gentoo penguins are only found on Biscoe Island. Based on this visual, it appears that island may be a good variable to use when predicting penguin species.\n\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis visualization shows the distribution of penguins according to their Culmen Length and Culmen Depth. As seen on the graph, the penguins are grouped into distinct groups by species, indicating that the culmen measurements will be good variables when predicting penguin species.\n\ntrain.groupby(\"Species\")[\"Clutch Completion\"].value_counts()\n\nSpecies                                    Clutch Completion\nAdelie Penguin (Pygoscelis adeliae)        Yes                  108\n                                           No                    12\nChinstrap penguin (Pygoscelis antarctica)  Yes                   47\n                                           No                    10\nGentoo penguin (Pygoscelis papua)          Yes                   90\n                                           No                     8\nName: count, dtype: int64\n\n\nThis table shows that each species has a similar proportion of Clutch Completions, indicating that it would not be a helpful variable in predicting penguin species.\n\ntrain.groupby('Species')[[\"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]].aggregate('mean')\n\n\n\n\n\n\n\n\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\nSpecies\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n8.861824\n-25.796897\n\n\nChinstrap penguin (Pygoscelis antarctica)\n9.331004\n-24.553401\n\n\nGentoo penguin (Pygoscelis papua)\n8.247341\n-26.149389\n\n\n\n\n\n\n\nThis table shows the mean Delta 15 N (o/oo) and the mean Delta 13 C (o/oo) for each species and shows that they are similar across all three, so neither of these values will likely make a good predictor variable.\n\n\nChoose Features\nAfter looking at graphs and tables of the variables included in this data set, the most likely to train an accurate predictive model seem to be Sex, Island, Culmen Length (mm), Culmen Depth (mm) and Body Mass (g). I trained a logistic regression model on each to which score the highest out of all the possible combinations.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    LR = LogisticRegression(max_iter = 5000)\n    LR.fit(X_train[cols], y_train)\n    print(LR.score(X_train[cols], y_train))\n    \n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.9921875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n0.98046875\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.77734375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n0.9765625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.8359375\n\n\nOut of these variables, the best models are the model using columns Sex, Culmen Length (mm) and Culmen Depth (mm) with a score of 0.992 and the model using Island, Culmen Length (mm) and Culmen Depth (mm), with a score of 0.996.\n\n\nMODEL\n\ncols = [\"Culmen Depth (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\n\nNow that I know that this combination yields the highest scoring logistic regression models, I am going to try to improve the training accuracy by using a few other models, and I will use cross validation for each to prevent overfitting.\n\nLogistic Regression\n\nfrom sklearn.model_selection import cross_val_score\n\nLR = LogisticRegression(max_iter = 5000)\nLR.fit(X_train[cols], y_train)\nprint(cross_val_score(LR, X_train[cols], y_train, cv = 5).mean())\n\n0.9843891402714933\n\n\n\n\nSupport Vector Machine\n\nfrom sklearn.svm import SVC\n\n\ngamma = 10.0**np.arange(-4, 4)\n\nfor i in gamma:\n    SVM = SVC(gamma = i)\n    print('gamma = ', i)\n    SVM.fit(X_train[cols], y_train)\n    print(cross_val_score(SVM, X_train[cols], y_train, cv = 5).mean())\n\ngamma =  0.0001\n0.7266214177978884\ngamma =  0.001\n0.9217948717948717\ngamma =  0.01\n0.9648567119155353\ngamma =  0.1\n0.9687782805429863\ngamma =  1.0\n0.9610105580693815\ngamma =  10.0\n0.7578431372549019\ngamma =  100.0\n0.4530920060331825\ngamma =  1000.0\n0.42971342383107086\n\n\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nmax_depth = np.arange(5, 15)\n\nfor i in max_depth:\n    print(\"max_depth: \", i)\n    RF = RandomForestClassifier(max_depth = i)\n    RF.fit(X_train[cols], y_train)\n    print(cross_val_score(RF, X_train[cols], y_train, cv = 5).mean())\n\nmax_depth:  5\n0.9843891402714933\nmax_depth:  6\n0.9765460030165911\nmax_depth:  7\n0.9804675716440423\nmax_depth:  8\n0.9804675716440423\nmax_depth:  9\n0.9804675716440423\nmax_depth:  10\n0.9804675716440423\nmax_depth:  11\n0.9804675716440423\nmax_depth:  12\n0.9804675716440423\nmax_depth:  13\n0.9843891402714933\nmax_depth:  14\n0.9766214177978885\n\n\nBased on these scores, the model that most accurately predicts penguin species is one of three:\nThe Logistic Regression Model with a cross-val score of 0.984, the Support Vector Machine model with a gamma values of 0.1 and a cross-val score of 0.969, and the Random Forest Classifier with a max-depth of 13 and a cross-val score of 0.984.\nBased off of this information, I decided to train both the logistic regression model and the random forest classifier to determine which to use.\n\nmax_depth = 13\nRF = RandomForestClassifier()\nRF.fit(X_train[cols], y_train)\nprint(RF.score(X_train[cols], y_train))\n\n1.0\n\n\n\nLR = LogisticRegression(max_iter = 1000)\nLR.fit(X_train[cols], y_train)\nprint(RF.score(X_train[cols], y_train))\n\n1.0\n\n\nBoth of these models have a training accuracy of 100%, so I decided to go with the Random Forest Classifier, although both appear to be equally as effective.\n\n\n\nEVALUATE\nNow that we have our trained model, with an accuracy of 1.0 on the training data, it is time to see how well it performs on testing data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nRF.score(X_test[cols], y_test)\n\n1.0\n\n\nOur model is 100% accurate on the test data.\nTo see more specifcially how our model classified these penguins, I will plot the decision regions using the following function.\n\nDecision Regions\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFirst I am plotting the decision regions of the training data.\n\nplot_regions(RF, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nAnd now the testing data\n\nplot_regions(RF, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nIt appears that the combination of understanding which islands certain species live on, in combination with knowing the dimensions of the penguins culmen makes a very effective prediction model.\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = RF.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSeeing as our model has 100% accuracy, the confusion matrix is not very informative for us. However, it does show us how the test data is split by species.\n\n\n\nDiscussion\nThis Blog Post documents the process of downloading a data set, exploring the data, training a model and evaluating that model. I was able to determine which variables would be best in predicting penguin species by first creating tables and plots to see the distribution of each variable, and then by brute force training Logistic Regression models on combinations of columns that I thought may be most predictive. After choosing Island, Culmen Length (mm) and Culmen Depth (mm) I trained different types of models (Logistic Regression, Support Vector Machine and Random Forest with different parameters) before ending up with a 100% training accuracy Random Forest model with a max_depth of 13. This model also yielded 100% test accuracy. In completing this Blog Post I learned about the steps and est practices of training a model using the existing frameworks and how to tweak models to improve their accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Palmer Pengiuns\n\n\n\n\n\nA blog post exploring the Palmer Penguiuns dataset and creating a model to classify penguin species\n\n\n\n\n\nMar 7, 2024\n\n\nLila Kosowsky\n\n\n\n\n\n\nNo matching items"
  }
]